{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import qkera_experiments.dataset as dataset\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from qkeras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = dataset.load_cifar100()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetPaperLR(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr = 0.1, learning_rate_decay=10, steps=[32000, 48000]):\n",
    "        super(ResNetPaperLR, self).__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.learning_rate_decay = learning_rate_decay\n",
    "        self.steps = steps\n",
    "        self.current_lr = 0\n",
    "    def __call__(self, step):        \n",
    "        step12 = tf.where(step < self.steps[1], self.initial_lr/self.learning_rate_decay, self.initial_lr/(self.learning_rate_decay**2))    \n",
    "        step01 = tf.where(step < self.steps[0], self.initial_lr, step12)\n",
    "        return step01\n",
    "                         \n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_lr\": self.initial_lr,\n",
    "            \"learning_rate_decay\": self.learning_rate_decay,\n",
    "            \"steps\": self.steps\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the quantized model model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.utils import load_qmodel\n",
    "\n",
    "\n",
    "qmodel = load_qmodel(\"qmodels/resnet32/model.h5\", custom_objects={\"ResNetPaperLR\":ResNetPaperLR})\n",
    "qmodel.evaluate(x_test, y_test)\n",
    "\n",
    "x_in = qmodel.layers[0].input\n",
    "x_out = qmodel.layers[-2].output\n",
    "\n",
    "base_model = keras.Model(inputs=[x_in], outputs=[x_out])\n",
    "base_model.summary()\n",
    "\n",
    "del qmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# including base model as a preprocessing component of the Deep Nibble MLP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the dataset\n",
    "x_train_2 = base_model.predict(x_train)\n",
    "print(x_train_2.shape)\n",
    "# preprocess the dataset\n",
    "x_test_2 = base_model.predict(x_test)\n",
    "print(x_test_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training Deep Nibble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mlp_from_scratch.NeuralNetwork import QNeuralNetworkWithScale\n",
    "\n",
    "\n",
    "# Define neural network inputnoutput\n",
    "input_size = x_train_2.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "print(x_train_2.shape, x_test_2.shape, y_train.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_nibble_history = []\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    # Create and train the neural network\n",
    "    neural_network_with_scale = QNeuralNetworkWithScale(input_size, output_size)\n",
    "\n",
    "    # train the nn\n",
    "    neural_network_with_scale.train(x_train_2, y_train, learning_rate=0.0000100, num_epochs=1, batch_size=256, x_val=x_test_2, y_val=y_test)\n",
    "    neural_network_with_scale.train(x_train_2, y_train, learning_rate=0.000100, num_epochs=4, batch_size=256, x_val=x_test_2, y_val=y_test)\n",
    "    neural_network_with_scale.train(x_train_2, y_train, learning_rate=0.000010, num_epochs=5, batch_size=256, x_val=x_test_2, y_val=y_test)\n",
    "\n",
    "    neural_network_with_scale.save_weights(f\"dn_models/deep_nibble_model_{i}\")\n",
    "\n",
    "    np.save(f\"dn_models/deep_nibble_model_{i}/loss_hist\", np.array(neural_network_with_scale.loss_hist))\n",
    "    np.save(f\"dn_models/deep_nibble_model_{i}/acc_hist\", np.array(neural_network_with_scale.acc_hist))\n",
    "\n",
    "    deep_nibble_history.append((neural_network_with_scale.loss_hist , neural_network_with_scale.acc_hist))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "  if epoch < 1:\n",
    "    return 0.001\n",
    "  elif epoch < 5:\n",
    "    return 0.01\n",
    "  else:\n",
    "    return 0.001\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "fp_history = []\n",
    "\n",
    "for i in range(10):\n",
    "  x = x_in = keras.layers.Input((64))\n",
    "  x = keras.layers.Dense(units=256, activation=\"relu\")(x)\n",
    "  x = keras.layers.Dense(units=256, activation=\"relu\")(x)\n",
    "  x = keras.layers.Dense(units=10)(x)\n",
    "  fp_model = keras.Model(inputs=x_in, outputs=x)\n",
    "  fp_model.compile(keras.optimizers.SGD(learning_rate=0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "  fp_hist = fp_model.fit(x_train_2, y_train, batch_size=256, epochs=10, validation_data=(x_test_2, y_test), callbacks=[callback])\n",
    "\n",
    "  fp_model.save(f\"fp_models/model_{i}.h5\")\n",
    "  np.save(f\"fp_models/model_{i}_acc_hist\",  np.array(fp_hist.history[\"val_loss\"]))\n",
    "  np.save(f\"fp_models/model_{i}_loss_hist\",  np.array(fp_hist.history[\"val_accuracy\"]))\n",
    "\n",
    "  fp_history.append((fp_hist.history[\"val_loss\"], fp_hist.history[\"val_accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training po2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "  if epoch < 1:\n",
    "    return 0.0001\n",
    "  elif epoch < 5:\n",
    "    return 0.001\n",
    "  else:\n",
    "    return 0.0001\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "po2_history = []\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "  x = x_in = keras.layers.Input((64))\n",
    "  x = QActivation(activation=\"quantized_relu_po2(4,1, use_stochastic_rounding=True)\")(x)\n",
    "  x = QDense(units=256, kernel_quantizer=quantized_po2(4,1, use_stochastic_rounding=True), bias_quantizer=(quantized_po2(4,1, use_stochastic_rounding=True)))(x)\n",
    "  x = QActivation(activation=\"quantized_relu_po2(4,1, use_stochastic_rounding=True)\")(x)\n",
    "  x = QDense(units=256, kernel_quantizer=quantized_po2(4,1, use_stochastic_rounding=True), bias_quantizer=(quantized_po2(4,1, use_stochastic_rounding=True)))(x)\n",
    "  x = QActivation(activation=\"quantized_relu_po2(4,1, use_stochastic_rounding=True)\")(x)\n",
    "  x = QDense(units=10, kernel_quantizer=quantized_po2(4,1, use_stochastic_rounding=True), bias_quantizer=(quantized_po2(4,1, use_stochastic_rounding=True)))(x)\n",
    "  # x = QActivation(activation=quantized_bits(4, 0, 1, use_stochastic_rounding=False))(x)\n",
    "  po2_model = keras.Model(inputs=x_in, outputs=x)\n",
    "  po2_model.compile(keras.optimizers.SGD(learning_rate=0.0001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "  po2_hist = po2_model.fit(x_train_2, y_train, batch_size=256, epochs=10, validation_data=(x_test_2, y_test), callbacks=[callback])\n",
    "\n",
    "  po2_model.save(f\"po2_models/model_{i}.h5\")\n",
    "  np.save(f\"po2_models/model_{i}_acc_hist\",  np.array(po2_hist.history[\"val_accuracy\"]))\n",
    "  np.save(f\"po2_models/model_{i}_loss_hist\",  np.array(po2_hist.history[\"val_loss\"]))\n",
    "\n",
    "\n",
    "  po2_history.append((po2_hist.history[\"val_loss\"], po2_hist.history[\"val_accuracy\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(10):\n",
    "    dn_loss = deep_nibble_history[i][0]\n",
    "    \n",
    "    plt.plot(fp_history[i][0], label='FP32', color=\"red\")\n",
    "    plt.plot(po2_history[i][0], label='Po2 4-bits', color=\"blue\")\n",
    "    plt.plot(dn_loss, label='Deep Nibble', color=\"green\")    \n",
    "    plt.title('Validation Loss')\n",
    "\n",
    "# plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    dn_acc = deep_nibble_history[i][1]\n",
    "        \n",
    "    plt.plot(fp_history[i][1], label='FP32', color=\"red\")\n",
    "    plt.plot(po2_history[i][1], label='Po2 4-bits', color=\"blue\")\n",
    "    plt.plot(dn_acc, label='Deep Nibble', color=\"green\")    \n",
    "    plt.title('Validation Accuracy')\n",
    "\n",
    "#plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn_acc = np.mean(np.array(deep_nibble_history)[:,1,:], axis=0)\n",
    "fp_acc = np.mean(np.array(fp_history)[:,1,:], axis=0)\n",
    "po2_acc = np.mean(np.array(po2_history)[:,1,:], axis=0)\n",
    "\n",
    "plt.plot(fp_acc, label='FP32', color=\"red\")\n",
    "plt.plot(po2_acc, label='Po2 4-bits', color=\"blue\")\n",
    "plt.plot(dn_acc, label='Deep Nibble', color=\"green\")    \n",
    "plt.title('Validation Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(fp_acc))\n",
    "print(np.max(dn_acc))\n",
    "print(np.max(po2_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#plt.plot(history.history[\"loss\"])\n",
    "plt.plot(fp_hist.history[\"val_loss\"], label='FP32')\n",
    "plt.plot(po2_hist.history[\"val_loss\"], label='Po2 4-bits')\n",
    "plt.plot(neural_network_with_scale.loss_hist, label='Deep Nibble')\n",
    "plt.legend()\n",
    "plt.title('Validation Loss')\n",
    "plt.show()    \n",
    "#plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(fp_hist.history[\"val_accuracy\"], label='FP32')\n",
    "plt.plot(po2_hist.history[\"val_accuracy\"],  label='Po2 4-bits')\n",
    "plt.plot(neural_network_with_scale.acc_hist, label='Deep Nibble')\n",
    "plt.legend()\n",
    "plt.title('Validation Accuracy Cifar10')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy Top-1')\n",
    "plt.xticks(range(0, 20, 2))\n",
    "\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(neural_network_with_scale.acc_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
