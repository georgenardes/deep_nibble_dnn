{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train VGG-16 on CIFAR10 using fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dataset import load_mnist, load_cifar10\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from NeuralNetwork import QLeNet, QNeuralNetworkWithScale\n",
    "import Activations\n",
    "import FullyConnectedLayer \n",
    "import qvgg16\n",
    "from qkeras import *\n",
    "from qkeras.utils import model_quantize, load_qmodel\n",
    "\n",
    "\n",
    "# load dataset\n",
    "x_train, y_train, x_test, y_test = load_cifar10()\n",
    "\n",
    "# vgg_pretrained = keras.models.load_model(\"vgg_model_fp32.h5\", compile=True)\n",
    "# metrics = vgg_pretrained.evaluate(x_test, y_test)\n",
    "# vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetuning with deep nibble without stochastic zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_in = keras.layers.Input((32,32,3))\n",
    "x = keras.layers.ReLU(name=\"relu_input\")(x)\n",
    "for l in vgg_pretrained.layers[1:]:\n",
    "    x = l(x)\n",
    "\n",
    "vgg = keras.Model(inputs=[x_in], outputs=[x])\n",
    "# vgg.summary()\n",
    "\n",
    "\n",
    "vgg.compile(optimizer=keras.optimizers.Adam(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.02), metrics=[\"accuracy\"])\n",
    "\n",
    "quantizer_config = {        \n",
    "    \"QConv2D\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QDense\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QActivation\": { \"relu\": \"quantized_relu_po2(4,1,use_stochastic_rounding=True)\" },    \n",
    "}\n",
    "\n",
    "qvgg = model_quantize(vgg, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "qvgg.summary()\n",
    "\n",
    "qvgg.compile(optimizer=keras.optimizers.Adam(0.0001), loss=keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.02), metrics=[\"accuracy\"])\n",
    "hist = qvgg.fit(x_train, y_train, 256, 5, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qvgg.save(\"qvgg_model_fp32.h5\")\n",
    "qvgg = load_qmodel(\"qvgg_model_fp32.h5\")\n",
    "qvgg.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot weight distribuiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for l in qvgg.layers:\n",
    "    if isinstance(l, QConv2D) or isinstance(l, QDense):\n",
    "        w = l.weights[0].numpy()\n",
    "        b = l.weights[1].numpy()\n",
    "        \n",
    "        qw = get_quantizer(l.kernel_quantizer)(w)\n",
    "        qb = get_quantizer(l.kernel_quantizer)(b)\n",
    "\n",
    "        print(np.min(np.abs(qw)))\n",
    "\n",
    "        fig, ax = plt.subplots(1, 2)\n",
    "        ax[0].hist(np.ravel(qw), bins=128)\n",
    "        ax[1].hist(np.ravel(qb), bins=128)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import quantizer\n",
    "\n",
    "from ConvLayer import *\n",
    "\n",
    "dn_layers = []\n",
    "for l in vgg.layers:\n",
    "    if isinstance(l, keras.layers.Conv2D):    \n",
    "        print(\"instanciating conv layer...\")\n",
    "        l.weights[0].shape[0],l.weights[0].shape[1]\n",
    "\n",
    "        w_shape = l.weights[0].shape\n",
    "        nfilters = w_shape[3]\n",
    "        kernel_size = w_shape[0]\n",
    "        input_channels = w_shape[2]\n",
    "        strides=[1,1,1,1] ### TODO: variable strides\n",
    "        padding= l.padding\n",
    "\n",
    "        # create QCONVLAYER\n",
    "        qfc = QConvLayer(nfilters, kernel_size, input_channels, strides, padding)\n",
    "        \n",
    "        fpw = l.weights[0].numpy()        \n",
    "        fpb  = l.weights[1].numpy()\n",
    "        \n",
    "        w_scale = np.max(np.abs(fpw))\n",
    "        \n",
    "        fpw_scaled = fpw / w_scale\n",
    "        qw = quantizer.quantize(fpw_scaled, True, False)\n",
    "        \n",
    "        # atribui o peso quantizado\n",
    "        qfc.qw = qw\n",
    "        qfc.weights_scale = fpw_scaled\n",
    "               \n",
    "\n",
    "        plt.hist(np.ravel(qw), bins=64)\n",
    "        plt.hist(np.ravel(fpw_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpw), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        fpb_scaled = fpb / w_scale\n",
    "        qb = quantizer.quantize(fpb_scaled, True, False)\n",
    "        qfc.qb = qb\n",
    "        plt.hist(np.ravel(qb), bins=64)\n",
    "        plt.hist(np.ravel(fpb_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpb), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        dn_layers.append(qfc)\n",
    "\n",
    "    if isinstance(l, keras.layers.MaxPool2D):    \n",
    "        print(l)\n",
    "        dn_maxpool = CustomMaxPool(l.pool_size, l.strides)\n",
    "        dn_layers.append(dn_maxpool)\n",
    "\n",
    "    if isinstance(l, keras.layers.Flatten):    \n",
    "        dn_layers.append(CustomFlatten(l.input_shape))\n",
    "    if isinstance(l, keras.layers.Dense):        \n",
    "        \n",
    "        qfc = FullyConnectedLayer.QFullyConnectedLayerWithScale(l.weights[0].shape[0],l.weights[0].shape[1])\n",
    "        \n",
    "        fpw = l.weights[0].numpy()        \n",
    "        fpb  = l.weights[1].numpy()\n",
    "        \n",
    "        w_scale = np.max(np.abs(fpw))\n",
    "        \n",
    "        fpw_scaled = fpw / w_scale\n",
    "        qw = quantizer.quantize(fpw_scaled, True, False)\n",
    "        \n",
    "        # atribui o peso quantizado\n",
    "        qfc.qw = qw\n",
    "        qfc.weights_scale = fpw_scaled\n",
    "               \n",
    "\n",
    "        plt.hist(np.ravel(qw), bins=64)\n",
    "        plt.hist(np.ravel(fpw_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpw), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        fpb_scaled = fpb / w_scale\n",
    "        qb = quantizer.quantize(fpb_scaled, True, False)\n",
    "        qfc.qb = qb\n",
    "        plt.hist(np.ravel(qb), bins=64)\n",
    "        plt.hist(np.ravel(fpb_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpb), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        dn_layers.append(qfc)\n",
    "\n",
    "\n",
    "    if isinstance(l, keras.layers.ReLU):                \n",
    "        dn_layers.append(Activations.QReLU())\n",
    "\n",
    "print(dn_layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep nibble direct quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qlenet = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)\n",
    "\n",
    "iter = 20\n",
    "mean_acc = 0\n",
    "for i in range(iter): \n",
    "    qlenet.load_layers_from_model(qvgg, from_layer=0)\n",
    "    y_pred = qlenet.predict(x_test, 256)            \n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    print(f\"Accuracy: {accuracy * 100}\")\n",
    "\n",
    "    mean_acc += accuracy\n",
    "print(f\"mean Accuracy: {mean_acc * 100/iter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep nibble finetunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # preprocess\n",
    "\n",
    "# create backbone\n",
    "x_in = qvgg.layers[0].output\n",
    "x = qvgg.layers[19].output\n",
    "back_bone = keras.models.Model(inputs=[x_in], outputs=[x])\n",
    "back_bone.summary()\n",
    "\n",
    "# preprocess the dataset with Deep Nibble quantized VGG\n",
    "x_train_2 = back_bone.predict(x_train, 256)\n",
    "x_test_2 = back_bone.predict(x_test, 256)\n",
    "print(x_train_2.shape)\n",
    "\n",
    "# # quantize backbone to deep nibble\n",
    "# input_shape = x_train.shape[1:]\n",
    "# qvgg_backbone = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)\n",
    "# qvgg_backbone.load_layers_from_model(back_bone)\n",
    "# \n",
    "# # preprocess the dataset with Deep Nibble quantized VGG\n",
    "# x_train_2 = qvgg_backbone.predict(x_train, 256, apply_argmax=False)\n",
    "# x_test_2 = qvgg_backbone.predict(x_test, 256, apply_argmax=False)\n",
    "# \n",
    "# \n",
    "# del x_train, x_test, back_bone, qvgg_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"x_train_preprocessed\", x_train_2)\n",
    "np.save(\"x_test_preprocessed\", x_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096 256\n",
      "256 256\n",
      "256 10\n",
      "iteration 0 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.014873555861413479 Accuracy: 54.38999938964844%\n",
      "Epoch 1/1, Loss: 0.003267445135861635 Accuracy: 68.91000366210938%\n",
      "Accuracy: 68.7699966430664%\n",
      "iteration 1 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.005771460477262735 Accuracy: 68.81999969482422%\n",
      "Epoch 1/1, Loss: 0.003688649507239461 Accuracy: 67.16999816894531%\n",
      "Accuracy: 67.37999725341797%\n",
      "iteration 2 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.008524077944457531 Accuracy: 68.06999969482422%\n",
      "Epoch 1/1, Loss: 0.0063020470552146435 Accuracy: 66.72999572753906%\n",
      "Accuracy: 66.04000091552734%\n",
      "iteration 3 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.01318732462823391 Accuracy: 66.25999450683594%\n",
      "Epoch 1/1, Loss: 0.006914219353348017 Accuracy: 66.25%\n",
      "Accuracy: 66.75999450683594%\n",
      "iteration 4 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.015567622147500515 Accuracy: 66.5%\n",
      "Epoch 1/1, Loss: 0.01179305650293827 Accuracy: 65.16999816894531%\n",
      "Accuracy: 64.9000015258789%\n",
      "iteration 5 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.026211291551589966 Accuracy: 65.52999877929688%\n",
      "Epoch 1/1, Loss: 0.015047888271510601 Accuracy: 65.4000015258789%\n",
      "Accuracy: 65.41999816894531%\n",
      "iteration 6 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.03208097815513611 Accuracy: 64.7800064086914%\n",
      "Epoch 1/1, Loss: 0.017586112022399902 Accuracy: 63.090003967285156%\n",
      "Accuracy: 62.980003356933594%\n",
      "iteration 7 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.03648874908685684 Accuracy: 63.459999084472656%\n",
      "Epoch 1/1, Loss: 0.019826307892799377 Accuracy: 63.62000274658203%\n",
      "Accuracy: 63.30999755859375%\n",
      "iteration 8 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.03855356574058533 Accuracy: 63.720001220703125%\n",
      "Epoch 1/1, Loss: 0.02262009121477604 Accuracy: 44.220001220703125%\n",
      "Accuracy: 44.78000259399414%\n",
      "iteration 9 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.03296779468655586 Accuracy: 33.77000045776367%\n",
      "Epoch 1/1, Loss: 0.010282129980623722 Accuracy: 15.75%\n",
      "Accuracy: 15.379999160766602%\n",
      "Accuracy: 58.571998596191406%\n"
     ]
    }
   ],
   "source": [
    "x_train = np.load(\"x_train_preprocessed.npy\")\n",
    "x_test = np.load(\"x_test_preprocessed.npy\")\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qvgg_mlp = QNeuralNetworkWithScale(input_size=4096, output_size=10)\n",
    "\n",
    "iters = 10\n",
    "mean_acc = 0\n",
    "for i in range(iters):\n",
    "    print(f\"iteration {i} ... \\n\\n\")\n",
    "    \n",
    "    # qvgg_mlp.load_layers_from_model(qvgg) ## carrega as camadas totalmente conectadas\n",
    "    # del qvgg\n",
    "\n",
    "    # finetune the dnn\n",
    "    qvgg_mlp.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, batch_size=128, x_val=x_test, y_val=y_test)\n",
    "    qvgg_mlp.train(x_train, y_train, learning_rate=0.000100, num_epochs=1, batch_size=256, x_val=x_test, y_val=y_test)\n",
    "        \n",
    "    \n",
    "    # predict finetuned\n",
    "    y_pred = qvgg_mlp.predict(x_test, 256)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    mean_acc += accuracy\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "mean_acc /= iters\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from FullyConnectedLayer import *\n",
    "from ConvLayer import *\n",
    "\n",
    "\n",
    "for i, layer in enumerate(qvgg_mlp.layers):\n",
    "    if isinstance(layer, QFullyConnectedLayerWithScale) or isinstance(layer, QConvLayer):\n",
    "        plt.figure(dpi=300)\n",
    "        plt.plot(np.clip(layer.ws_hist, 0, 2000000))\n",
    "        plt.plot(np.clip(layer.bs_hist, 0, 2000000))\n",
    "        plt.plot(np.clip(layer.os_hist, 0, 2000000))\n",
    "        plt.plot(np.clip(layer.gws_hist, 0, 2000000))\n",
    "        plt.plot(np.clip(layer.gbs_hist, 0, 2000000))\n",
    "        plt.plot(np.clip(layer.gos_hist, 0, 2000000))\n",
    "        \n",
    "        leg = [ \n",
    "                \"ws_hist\",\n",
    "                \"bs_hist\",\n",
    "                \"os_hist\",\n",
    "                \"gws_hist\",\n",
    "                \"gbs_hist\",\n",
    "                \"gos_hist\"\n",
    "         ]\n",
    "             \n",
    "             \n",
    "        plt.legend(leg)\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PO2 direct quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.utils import model_quantize, model_save_quantized_weights\n",
    "from qkeras import *\n",
    "\n",
    "\n",
    "# add one relu layer after input\n",
    "x = x_in = keras.layers.Input((32,32,3))\n",
    "x = keras.layers.ReLU(name=\"relu_input\")(x)\n",
    "for l in vgg.layers[1:]:\n",
    "    x = l(x)\n",
    "\n",
    "\n",
    "vgg = keras.Model(inputs=[x_in], outputs=[x])\n",
    "vgg.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "quantizer_config = {        \n",
    "    \"QConv2D\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QDense\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QActivation\": { \"relu\": \"quantized_relu_po2(4,1,use_stochastic_rounding=True)\" },    \n",
    "}\n",
    "\n",
    "qmodel2 = model_quantize(vgg, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "qmodel2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(vgg, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    \n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "\n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PO2 finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(lenet, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "    \n",
    "    \n",
    "    for l in qmodel2.layers:\n",
    "        if isinstance(l, QConv2D):\n",
    "            l.trainable = False\n",
    "\n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)            \n",
    "\n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "\n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training last layers deep nibble from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qlenet = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)\n",
    "\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    print(f\"iteration {i} ... \\n\\n\")\n",
    "    \n",
    "    # load pre-trained model\n",
    "    qlenet.load_layers_from_model(lenet)\n",
    "    qlenet.freeze_conv = True\n",
    "    # restart\n",
    "    qlenet.restart_fc_layers()\n",
    "    \n",
    "\n",
    "    # finetune the dnn\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, x_val=x_test, y_val=y_test)\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000100, num_epochs=10, x_val=x_test, y_val=y_test)\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, x_val=x_test, y_val=y_test)\n",
    "    \n",
    "    \n",
    "    # predict finetuned\n",
    "    y_pred = qlenet.predict(x_test, 256)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    mean_acc += accuracy\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training last layer po2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.utils import model_quantize, model_save_quantized_weights\n",
    "from qkeras import *\n",
    "\n",
    "\n",
    "vgg = keras.models.load_model(\"vgg_model_fp32.h5\", compile=False)\n",
    "\n",
    "# add one relu layer after input\n",
    "x = x_in = keras.layers.Input((32,32,3))\n",
    "x = keras.layers.ReLU(name=\"relu_input\")(x)\n",
    "for l in vgg.layers[1:]:\n",
    "    x = l(x)\n",
    "\n",
    "\n",
    "vgg = keras.Model(inputs=[x_in], outputs=[x])\n",
    "vgg.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "quantizer_config = {        \n",
    "    \"QConv2D\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QDense\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QActivation\": { \"relu\": \"quantized_relu_po2(4,1,use_stochastic_rounding=True)\" },    \n",
    "}\n",
    "\n",
    "qmodel2 = model_quantize(vgg, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "qmodel2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    print(\"iteratoin\", i, \"...\\n\")\n",
    "\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(vgg, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "    \n",
    "    # freeze and restart layer weights\n",
    "    for l in qmodel2.layers:\n",
    "        if isinstance(l, QConv2D):\n",
    "            l.trainable = False\n",
    "        if isinstance(l, QDense):            \n",
    "            w = keras.initializers.GlorotNormal()(l.weights[0].shape)            \n",
    "            b = tf.zeros_like(l.weights[1])            \n",
    "            l.set_weights([w, b])\n",
    "\n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)                \n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=10, validation_data=(x_test, y_test), validation_freq=1)                \n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)                \n",
    "\n",
    "\n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "    \n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
