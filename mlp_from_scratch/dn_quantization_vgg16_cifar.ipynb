{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train VGG-16 on CIFAR10 using fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dataset import load_mnist, load_cifar10\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from NeuralNetwork import QLeNet\n",
    "import Activations\n",
    "import FullyConnectedLayer \n",
    "import qvgg16\n",
    "\n",
    "\n",
    "# load dataset\n",
    "x_train, y_train, x_test, y_test = load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_in = keras.layers.Input((32,32,3))\n",
    "\n",
    "x = qvgg16.VGG_16(x_in, 10)\n",
    "\n",
    "vgg = keras.Model(inputs=[x_in], outputs=[x])\n",
    "vgg.summary()\n",
    "vgg.compile(optimizer=keras.optimizers.Adam(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.02), metrics=[\"accuracy\"])\n",
    "hist = vgg.fit(x_train, y_train, 256, 1, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot weight distribuiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for l in vgg.layers:\n",
    "    if isinstance(l, keras.layers.Conv2D) or isinstance(l, keras.layers.Dense):\n",
    "        w = l.weights[0].numpy()\n",
    "        b = l.weights[1].numpy()\n",
    "\n",
    "        plt.hist(np.ravel(w), bins=64)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como vincular um modelo com o outro?\n",
    "# mais vale criar um modelo do 0 com base no modelo q vier...\n",
    "# o problema maior é criar cada camada\n",
    "# a rede é só um array de camadas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import quantizer\n",
    "\n",
    "from ConvLayer import *\n",
    "\n",
    "dn_layers = []\n",
    "for l in vgg.layers:\n",
    "    if isinstance(l, keras.layers.Conv2D):    \n",
    "        print(\"instanciating conv layer...\")\n",
    "        l.weights[0].shape[0],l.weights[0].shape[1]\n",
    "\n",
    "        w_shape = l.weights[0].shape\n",
    "        nfilters = w_shape[3]\n",
    "        kernel_size = w_shape[0]\n",
    "        input_channels = w_shape[2]\n",
    "        strides=[1,1,1,1] ### TODO: variable strides\n",
    "        padding= l.padding\n",
    "\n",
    "        # create QCONVLAYER\n",
    "        qfc = QConvLayer(nfilters, kernel_size, input_channels, strides, padding)\n",
    "        \n",
    "        fpw = l.weights[0].numpy()        \n",
    "        fpb  = l.weights[1].numpy()\n",
    "        \n",
    "        w_scale = np.max(np.abs(fpw))\n",
    "        \n",
    "        fpw_scaled = fpw / w_scale\n",
    "        qw = quantizer.quantize(fpw_scaled, True, False)\n",
    "        \n",
    "        # atribui o peso quantizado\n",
    "        qfc.qw = qw\n",
    "        qfc.weights_scale = fpw_scaled\n",
    "               \n",
    "\n",
    "        plt.hist(np.ravel(qw), bins=64)\n",
    "        plt.hist(np.ravel(fpw_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpw), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        fpb_scaled = fpb / w_scale\n",
    "        qb = quantizer.quantize(fpb_scaled, True, False)\n",
    "        qfc.qb = qb\n",
    "        plt.hist(np.ravel(qb), bins=64)\n",
    "        plt.hist(np.ravel(fpb_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpb), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        dn_layers.append(qfc)\n",
    "\n",
    "    if isinstance(l, keras.layers.MaxPool2D):    \n",
    "        print(l)\n",
    "        dn_maxpool = CustomMaxPool(l.pool_size, l.strides)\n",
    "        dn_layers.append(dn_maxpool)\n",
    "\n",
    "    if isinstance(l, keras.layers.Flatten):    \n",
    "        dn_layers.append(CustomFlatten(l.input_shape))\n",
    "    if isinstance(l, keras.layers.Dense):        \n",
    "        \n",
    "        qfc = FullyConnectedLayer.QFullyConnectedLayerWithScale(l.weights[0].shape[0],l.weights[0].shape[1])\n",
    "        \n",
    "        fpw = l.weights[0].numpy()        \n",
    "        fpb  = l.weights[1].numpy()\n",
    "        \n",
    "        w_scale = np.max(np.abs(fpw))\n",
    "        \n",
    "        fpw_scaled = fpw / w_scale\n",
    "        qw = quantizer.quantize(fpw_scaled, True, False)\n",
    "        \n",
    "        # atribui o peso quantizado\n",
    "        qfc.qw = qw\n",
    "        qfc.weights_scale = fpw_scaled\n",
    "               \n",
    "\n",
    "        plt.hist(np.ravel(qw), bins=64)\n",
    "        plt.hist(np.ravel(fpw_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpw), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        fpb_scaled = fpb / w_scale\n",
    "        qb = quantizer.quantize(fpb_scaled, True, False)\n",
    "        qfc.qb = qb\n",
    "        plt.hist(np.ravel(qb), bins=64)\n",
    "        plt.hist(np.ravel(fpb_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpb), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        dn_layers.append(qfc)\n",
    "\n",
    "\n",
    "    if isinstance(l, keras.layers.ReLU):                \n",
    "        dn_layers.append(Activations.QReLU())\n",
    "\n",
    "print(dn_layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep nibble direct quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg.save(\"vgg_model_fp32.h5\")\n",
    "# load pre-trained\n",
    "vgg = keras.models.load_model(\"vgg_model_fp32.h5\", compile=False)\n",
    "#vgg.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qlenet = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(100): \n",
    "    qlenet.load_layers_from_model(vgg)\n",
    "    y_pred = qlenet.predict(x_test, 256)\n",
    "        \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "    mean_acc += accuracy\n",
    "print(f\"mean Accuracy: {mean_acc * 100/100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep nibble finetunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess x\n",
    "\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qlenet = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qlenet = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)\n",
    "\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    print(f\"iteration {i} ... \\n\\n\")\n",
    "    \n",
    "    qlenet.load_layers_from_model(vgg)\n",
    "    qlenet.batch_size = 256\n",
    "    qlenet.freeze_conv = True\n",
    "\n",
    "    # finetune the dnn\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, x_val=x_test, y_val=y_test)\n",
    "    \n",
    "    \n",
    "    # predict finetuned\n",
    "    y_pred = qlenet.predict(x_test, 256)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    mean_acc += accuracy\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PO2 direct quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.utils import model_quantize, model_save_quantized_weights\n",
    "from qkeras import *\n",
    "\n",
    "\n",
    "# add one relu layer after input\n",
    "x = x_in = keras.layers.Input((32,32,3))\n",
    "x = keras.layers.ReLU(name=\"relu_input\")(x)\n",
    "for l in vgg.layers[1:]:\n",
    "    x = l(x)\n",
    "\n",
    "\n",
    "vgg = keras.Model(inputs=[x_in], outputs=[x])\n",
    "vgg.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "quantizer_config = {        \n",
    "    \"QConv2D\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QDense\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QActivation\": { \"relu\": \"quantized_relu_po2(4,1,use_stochastic_rounding=True)\" },    \n",
    "}\n",
    "\n",
    "qmodel2 = model_quantize(vgg, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "qmodel2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(vgg, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    \n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "\n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PO2 finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(lenet, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "    \n",
    "    \n",
    "    for l in qmodel2.layers:\n",
    "        if isinstance(l, QConv2D):\n",
    "            l.trainable = False\n",
    "\n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)            \n",
    "\n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "\n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training last layers deep nibble from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qlenet = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)\n",
    "\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    print(f\"iteration {i} ... \\n\\n\")\n",
    "    \n",
    "    # load pre-trained model\n",
    "    qlenet.load_layers_from_model(lenet)\n",
    "    qlenet.freeze_conv = True\n",
    "    # restart\n",
    "    qlenet.restart_fc_layers()\n",
    "    \n",
    "\n",
    "    # finetune the dnn\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, x_val=x_test, y_val=y_test)\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000100, num_epochs=10, x_val=x_test, y_val=y_test)\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, x_val=x_test, y_val=y_test)\n",
    "    \n",
    "    \n",
    "    # predict finetuned\n",
    "    y_pred = qlenet.predict(x_test, 256)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    mean_acc += accuracy\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training last layer po2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\georg\\Desktop\\mestrado_luiz\\deep_nibble_dnn\\.venv\\lib\\site-packages\\keras\\initializers\\initializers_v2.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " relu_input (QActivation)    (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      " conv2d (QConv2D)            (None, 32, 32, 64)        1792      \n",
      "                                                                 \n",
      " re_lu (QActivation)         (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (QConv2D)          (None, 32, 32, 64)        36928     \n",
      "                                                                 \n",
      " re_lu_1 (QActivation)       (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (QConv2D)          (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " re_lu_2 (QActivation)       (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_3 (QConv2D)          (None, 16, 16, 128)       147584    \n",
      "                                                                 \n",
      " re_lu_3 (QActivation)       (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (QConv2D)          (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " re_lu_4 (QActivation)       (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_5 (QConv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " re_lu_5 (QActivation)       (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " conv2d_6 (QConv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " re_lu_6 (QActivation)       (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (QDense)              (None, 256)               1048832   \n",
      "                                                                 \n",
      " re_lu_7 (QActivation)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (QDense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " re_lu_8 (QActivation)       (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (QDense)            (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,852,682\n",
      "Trainable params: 2,852,682\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from qkeras.utils import model_quantize, model_save_quantized_weights\n",
    "from qkeras import *\n",
    "\n",
    "\n",
    "vgg = keras.models.load_model(\"vgg_model_fp32.h5\", compile=False)\n",
    "\n",
    "# add one relu layer after input\n",
    "x = x_in = keras.layers.Input((32,32,3))\n",
    "x = keras.layers.ReLU(name=\"relu_input\")(x)\n",
    "for l in vgg.layers[1:]:\n",
    "    x = l(x)\n",
    "\n",
    "\n",
    "vgg = keras.Model(inputs=[x_in], outputs=[x])\n",
    "vgg.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "quantizer_config = {        \n",
    "    \"QConv2D\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QDense\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QActivation\": { \"relu\": \"quantized_relu_po2(4,1,use_stochastic_rounding=True)\" },    \n",
    "}\n",
    "\n",
    "qmodel2 = model_quantize(vgg, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "qmodel2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteratoin 0 ...\n",
      "\n",
      "196/196 [==============================] - 27s 118ms/step - loss: 2.2944 - accuracy: 0.1272 - val_loss: 2.2601 - val_accuracy: 0.1644\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 25s 118ms/step - loss: 1.8582 - accuracy: 0.4383 - val_loss: 1.4588 - val_accuracy: 0.5799\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 1.2200 - accuracy: 0.6431 - val_loss: 1.1356 - val_accuracy: 0.6364\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.9966 - accuracy: 0.6869 - val_loss: 1.0184 - val_accuracy: 0.6568\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.8926 - accuracy: 0.7086 - val_loss: 0.9599 - val_accuracy: 0.6718\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8312 - accuracy: 0.7213 - val_loss: 0.9270 - val_accuracy: 0.6814\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7933 - accuracy: 0.7321 - val_loss: 0.9082 - val_accuracy: 0.6871\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7630 - accuracy: 0.7402 - val_loss: 0.8980 - val_accuracy: 0.6896\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7405 - accuracy: 0.7462 - val_loss: 0.8877 - val_accuracy: 0.6939\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7214 - accuracy: 0.7534 - val_loss: 0.8801 - val_accuracy: 0.6973\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7055 - accuracy: 0.7570 - val_loss: 0.8766 - val_accuracy: 0.6975\n",
      "196/196 [==============================] - 25s 117ms/step - loss: 0.6923 - accuracy: 0.7619 - val_loss: 0.8742 - val_accuracy: 0.6979\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.8742 - accuracy: 0.6979\n",
      "iteratoin 1 ...\n",
      "\n",
      "196/196 [==============================] - 25s 117ms/step - loss: 2.2821 - accuracy: 0.1254 - val_loss: 2.2478 - val_accuracy: 0.1627\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 24s 115ms/step - loss: 1.8336 - accuracy: 0.4748 - val_loss: 1.4557 - val_accuracy: 0.5837\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 1.2168 - accuracy: 0.6451 - val_loss: 1.1217 - val_accuracy: 0.6397\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.9831 - accuracy: 0.6875 - val_loss: 1.0098 - val_accuracy: 0.6612\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.8848 - accuracy: 0.7083 - val_loss: 0.9589 - val_accuracy: 0.6729\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.8293 - accuracy: 0.7208 - val_loss: 0.9251 - val_accuracy: 0.6818\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.7924 - accuracy: 0.7312 - val_loss: 0.9089 - val_accuracy: 0.6849\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7638 - accuracy: 0.7410 - val_loss: 0.8966 - val_accuracy: 0.6894\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 22s 110ms/step - loss: 0.7423 - accuracy: 0.7464 - val_loss: 0.8849 - val_accuracy: 0.6936\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7222 - accuracy: 0.7535 - val_loss: 0.8750 - val_accuracy: 0.6989\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7057 - accuracy: 0.7587 - val_loss: 0.8703 - val_accuracy: 0.7010\n",
      "196/196 [==============================] - 25s 118ms/step - loss: 0.6935 - accuracy: 0.7624 - val_loss: 0.8683 - val_accuracy: 0.7006\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.8683 - accuracy: 0.7006\n",
      "iteratoin 2 ...\n",
      "\n",
      "196/196 [==============================] - 25s 118ms/step - loss: 2.2765 - accuracy: 0.1366 - val_loss: 2.2430 - val_accuracy: 0.1788\n",
      "Epoch 1/10\n",
      "196/196 [==============================] - 25s 117ms/step - loss: 1.8414 - accuracy: 0.4515 - val_loss: 1.4572 - val_accuracy: 0.5755\n",
      "Epoch 2/10\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 1.2291 - accuracy: 0.6397 - val_loss: 1.1300 - val_accuracy: 0.6367\n",
      "Epoch 3/10\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.9949 - accuracy: 0.6868 - val_loss: 1.0110 - val_accuracy: 0.6601\n",
      "Epoch 4/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.8914 - accuracy: 0.7080 - val_loss: 0.9566 - val_accuracy: 0.6727\n",
      "Epoch 5/10\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.8304 - accuracy: 0.7205 - val_loss: 0.9270 - val_accuracy: 0.6827\n",
      "Epoch 6/10\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7915 - accuracy: 0.7321 - val_loss: 0.9072 - val_accuracy: 0.6886\n",
      "Epoch 7/10\n",
      "196/196 [==============================] - 22s 111ms/step - loss: 0.7622 - accuracy: 0.7396 - val_loss: 0.8955 - val_accuracy: 0.6928\n",
      "Epoch 8/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7385 - accuracy: 0.7472 - val_loss: 0.8864 - val_accuracy: 0.6967\n",
      "Epoch 9/10\n",
      "196/196 [==============================] - 22s 113ms/step - loss: 0.7175 - accuracy: 0.7534 - val_loss: 0.8796 - val_accuracy: 0.6980\n",
      "Epoch 10/10\n",
      "196/196 [==============================] - 22s 112ms/step - loss: 0.7019 - accuracy: 0.7586 - val_loss: 0.8729 - val_accuracy: 0.7002\n",
      "196/196 [==============================] - 25s 118ms/step - loss: 0.6891 - accuracy: 0.7627 - val_loss: 0.8709 - val_accuracy: 0.7003\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.8709 - accuracy: 0.7003\n",
      "iteratoin 3 ...\n",
      "\n",
      "196/196 [==============================] - 25s 117ms/step - loss: 2.2745 - accuracy: 0.1522 - val_loss: 2.2420 - val_accuracy: 0.1788\n",
      "Epoch 1/10\n",
      "195/196 [============================>.] - ETA: 0s - loss: 1.8405 - accuracy: 0.4629"
     ]
    }
   ],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    print(\"iteratoin\", i, \"...\\n\")\n",
    "\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(vgg, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "    \n",
    "    # freeze and restart layer weights\n",
    "    for l in qmodel2.layers:\n",
    "        if isinstance(l, QConv2D):\n",
    "            l.trainable = False\n",
    "        if isinstance(l, QDense):            \n",
    "            w = keras.initializers.GlorotNormal()(l.weights[0].shape)            \n",
    "            b = tf.zeros_like(l.weights[1])            \n",
    "            l.set_weights([w, b])\n",
    "\n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)                \n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=10, validation_data=(x_test, y_test), validation_freq=1)                \n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)                \n",
    "\n",
    "\n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "    \n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
