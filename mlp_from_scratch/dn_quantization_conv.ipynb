{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train lenet-5 on MNIST using fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dataset import load_mnist\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from NeuralNetwork import QLeNet\n",
    "import Activations\n",
    "import FullyConnectedLayer \n",
    "\n",
    "\n",
    "# load dataset\n",
    "x_train, y_train, x_test, y_test = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 28, 28, 16)        160       \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 28, 28, 16)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 32)        4640      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1568)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               401664    \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 474,826\n",
      "Trainable params: 474,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "235/235 [==============================] - 4s 5ms/step - loss: 0.4184 - accuracy: 0.9159 - val_loss: 0.2154 - val_accuracy: 0.9793\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.2019 - accuracy: 0.9834 - val_loss: 0.1850 - val_accuracy: 0.9860\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1784 - accuracy: 0.9894 - val_loss: 0.1716 - val_accuracy: 0.9894\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1658 - accuracy: 0.9928 - val_loss: 0.1684 - val_accuracy: 0.9902\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1594 - accuracy: 0.9948 - val_loss: 0.1649 - val_accuracy: 0.9907\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1538 - accuracy: 0.9963 - val_loss: 0.1616 - val_accuracy: 0.9921\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1497 - accuracy: 0.9969 - val_loss: 0.1605 - val_accuracy: 0.9924\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1457 - accuracy: 0.9980 - val_loss: 0.1589 - val_accuracy: 0.9914\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1433 - accuracy: 0.9983 - val_loss: 0.1600 - val_accuracy: 0.9906\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 4ms/step - loss: 0.1410 - accuracy: 0.9987 - val_loss: 0.1544 - val_accuracy: 0.9921\n"
     ]
    }
   ],
   "source": [
    "x = x_in = keras.layers.Input((28,28,1))\n",
    "\n",
    "x = keras.layers.Conv2D(16, kernel_size=3, padding=\"SAME\", kernel_initializer=\"glorot_normal\")(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D()(x)\n",
    "x = keras.layers.Conv2D(32, kernel_size=3, padding=\"SAME\", kernel_initializer=\"glorot_normal\")(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D()(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(256, kernel_initializer=\"glorot_normal\")(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Dense(256, kernel_initializer=\"glorot_normal\")(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Dense(10, kernel_initializer=\"glorot_normal\")(x)\n",
    "\n",
    "lenet = keras.Model(inputs=[x_in], outputs=[x])\n",
    "lenet.summary()\n",
    "lenet.compile(optimizer=keras.optimizers.Adam(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.02), metrics=[\"accuracy\"])\n",
    "hist = lenet.fit(x_train, y_train, 256, 10, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot weight distribuiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for l in lenet.layers:\n",
    "    if isinstance(l, keras.layers.Conv2D) or isinstance(l, keras.layers.Dense):\n",
    "        w = l.weights[0].numpy()\n",
    "        b = l.weights[1].numpy()\n",
    "\n",
    "        plt.hist(np.ravel(w), bins=64)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como vincular um modelo com o outro?\n",
    "# mais vale criar um modelo do 0 com base no modelo q vier...\n",
    "# o problema maior é criar cada camada\n",
    "# a rede é só um array de camadas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import quantizer\n",
    "\n",
    "from ConvLayer import *\n",
    "\n",
    "dn_layers = []\n",
    "for l in lenet.layers:\n",
    "    if isinstance(l, keras.layers.Conv2D):    \n",
    "        print(\"instanciating conv layer...\")\n",
    "        l.weights[0].shape[0],l.weights[0].shape[1]\n",
    "\n",
    "        w_shape = l.weights[0].shape\n",
    "        nfilters = w_shape[3]\n",
    "        kernel_size = w_shape[0]\n",
    "        input_channels = w_shape[2]\n",
    "        strides=[1,1,1,1] ### TODO: variable strides\n",
    "        padding= l.padding\n",
    "\n",
    "        # create QCONVLAYER\n",
    "        qfc = QConvLayer(nfilters, kernel_size, input_channels, strides, padding)\n",
    "        \n",
    "        fpw = l.weights[0].numpy()        \n",
    "        fpb  = l.weights[1].numpy()\n",
    "        \n",
    "        w_scale = np.max(np.abs(fpw))\n",
    "        \n",
    "        fpw_scaled = fpw / w_scale\n",
    "        qw = quantizer.quantize(fpw_scaled, True, False)\n",
    "        \n",
    "        # atribui o peso quantizado\n",
    "        qfc.qw = qw\n",
    "        qfc.weights_scale = fpw_scaled\n",
    "               \n",
    "\n",
    "        plt.hist(np.ravel(qw), bins=64)\n",
    "        plt.hist(np.ravel(fpw_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpw), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        fpb_scaled = fpb / w_scale\n",
    "        qb = quantizer.quantize(fpb_scaled, True, False)\n",
    "        qfc.qb = qb\n",
    "        plt.hist(np.ravel(qb), bins=64)\n",
    "        plt.hist(np.ravel(fpb_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpb), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        dn_layers.append(qfc)\n",
    "\n",
    "    if isinstance(l, keras.layers.MaxPool2D):    \n",
    "        print(l)\n",
    "        dn_maxpool = CustomMaxPool(l.pool_size, l.strides)\n",
    "        dn_layers.append(dn_maxpool)\n",
    "\n",
    "    if isinstance(l, keras.layers.Flatten):    \n",
    "        dn_layers.append(CustomFlatten(l.input_shape))\n",
    "    if isinstance(l, keras.layers.Dense):        \n",
    "        \n",
    "        qfc = FullyConnectedLayer.QFullyConnectedLayerWithScale(l.weights[0].shape[0],l.weights[0].shape[1])\n",
    "        \n",
    "        fpw = l.weights[0].numpy()        \n",
    "        fpb  = l.weights[1].numpy()\n",
    "        \n",
    "        w_scale = np.max(np.abs(fpw))\n",
    "        \n",
    "        fpw_scaled = fpw / w_scale\n",
    "        qw = quantizer.quantize(fpw_scaled, True, False)\n",
    "        \n",
    "        # atribui o peso quantizado\n",
    "        qfc.qw = qw\n",
    "        qfc.weights_scale = fpw_scaled\n",
    "               \n",
    "\n",
    "        plt.hist(np.ravel(qw), bins=64)\n",
    "        plt.hist(np.ravel(fpw_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpw), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        fpb_scaled = fpb / w_scale\n",
    "        qb = quantizer.quantize(fpb_scaled, True, False)\n",
    "        qfc.qb = qb\n",
    "        plt.hist(np.ravel(qb), bins=64)\n",
    "        plt.hist(np.ravel(fpb_scaled), bins=64)\n",
    "        plt.hist(np.ravel(fpb), bins=64)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        dn_layers.append(qfc)\n",
    "\n",
    "\n",
    "    if isinstance(l, keras.layers.ReLU):                \n",
    "        dn_layers.append(Activations.QReLU())\n",
    "\n",
    "print(dn_layers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep nibble direct quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qlenet = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(100): \n",
    "    qlenet.load_layers_from_model(lenet)\n",
    "    y_pred = qlenet.predict(x_test, 256)\n",
    "        \n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "    mean_acc += accuracy\n",
    "print(f\"mean Accuracy: {mean_acc * 100/100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mean Accuracy: {mean_acc * 100/100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep nibble finetunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qlenet = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)\n",
    "\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    print(f\"iteration {i} ... \\n\\n\")\n",
    "    \n",
    "    qlenet.load_layers_from_model(lenet)\n",
    "    \n",
    "\n",
    "    # finetune the dnn\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, x_val=x_test, y_val=y_test)\n",
    "    \n",
    "    \n",
    "    # predict finetuned\n",
    "    y_pred = qlenet.predict(x_test, 256)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    mean_acc += accuracy\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PO2 direct quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.utils import model_quantize, model_save_quantized_weights\n",
    "from qkeras import *\n",
    "\n",
    "\n",
    "# add one relu layer after input\n",
    "x = x_in = keras.layers.Input((28,28,1))\n",
    "x = keras.layers.ReLU()(x)\n",
    "for l in lenet.layers[1:]:\n",
    "    x = l(x)\n",
    "\n",
    "\n",
    "lenet = keras.Model(inputs=[x_in], outputs=[x])\n",
    "lenet.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "quantizer_config = {        \n",
    "    \"QConv2D\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QDense\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QActivation\": { \"relu\": \"quantized_relu_po2(4,1,use_stochastic_rounding=True)\" },    \n",
    "}\n",
    "\n",
    "qmodel2 = model_quantize(lenet, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "qmodel2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(lenet, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    \n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "\n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PO2 finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(lenet, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "    \n",
    "    \n",
    "    for l in qmodel2.layers:\n",
    "        if isinstance(l, QConv2D):\n",
    "            l.trainable = False\n",
    "\n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)            \n",
    "\n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "\n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training last layers deep nibble from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.003550273831933737 Accuracy: 85.75999736785889%\n",
      "Epoch 1/10, Loss: 0.002024939516559243 Accuracy: 92.94999837875366%\n",
      "Epoch 2/10, Loss: 0.004156340844929218 Accuracy: 94.1100001335144%\n",
      "Epoch 3/10, Loss: 0.007050109561532736 Accuracy: 93.68000030517578%\n",
      "Epoch 4/10, Loss: 0.00782956276088953 Accuracy: 93.16999912261963%\n",
      "Epoch 5/10, Loss: 0.008987232111394405 Accuracy: 94.84999775886536%\n",
      "Epoch 6/10, Loss: 0.009080170653760433 Accuracy: 94.98000144958496%\n",
      "Epoch 7/10, Loss: 0.007742258254438639 Accuracy: 95.4200029373169%\n",
      "Epoch 8/10, Loss: 0.006751822307705879 Accuracy: 95.99000215530396%\n",
      "Epoch 9/10, Loss: 0.006035152822732925 Accuracy: 96.14999890327454%\n",
      "Epoch 10/10, Loss: 0.0056358627043664455 Accuracy: 96.42000198364258%\n",
      "Epoch 1/1, Loss: 0.004233798943459988 Accuracy: 97.0300018787384%\n",
      "Accuracy: 97.25999450683594%\n",
      "iteration 1 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.004195638466626406 Accuracy: 84.53999757766724%\n",
      "Epoch 1/10, Loss: 0.0014226369094103575 Accuracy: 92.28000044822693%\n",
      "Epoch 2/10, Loss: 0.0024617870803922415 Accuracy: 93.19000244140625%\n",
      "Epoch 3/10, Loss: 0.005223182030022144 Accuracy: 94.19999718666077%\n",
      "Epoch 4/10, Loss: 0.007348326500505209 Accuracy: 94.09000277519226%\n",
      "Epoch 5/10, Loss: 0.009492021054029465 Accuracy: 94.58000063896179%\n",
      "Epoch 6/10, Loss: 0.008676963858306408 Accuracy: 95.56999802589417%\n",
      "Epoch 7/10, Loss: 0.007711875252425671 Accuracy: 95.42999863624573%\n",
      "Epoch 8/10, Loss: 0.006598776206374168 Accuracy: 95.49000263214111%\n",
      "Epoch 9/10, Loss: 0.005893431603908539 Accuracy: 96.43999934196472%\n",
      "Epoch 10/10, Loss: 0.005540898535400629 Accuracy: 96.46999835968018%\n",
      "Epoch 1/1, Loss: 0.004303413908928633 Accuracy: 97.10999727249146%\n",
      "Accuracy: 97.05999755859375%\n",
      "iteration 2 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.0033165859058499336 Accuracy: 86.44999861717224%\n",
      "Epoch 1/10, Loss: 0.0020960327237844467 Accuracy: 91.71000123023987%\n",
      "Epoch 2/10, Loss: 0.0039357272908091545 Accuracy: 94.24999952316284%\n",
      "Epoch 3/10, Loss: 0.006452749948948622 Accuracy: 92.33999848365784%\n",
      "Epoch 4/10, Loss: 0.008326656185090542 Accuracy: 93.14000010490417%\n",
      "Epoch 5/10, Loss: 0.009030098095536232 Accuracy: 94.87000107765198%\n",
      "Epoch 6/10, Loss: 0.009069419465959072 Accuracy: 95.20000219345093%\n",
      "Epoch 7/10, Loss: 0.007609045598655939 Accuracy: 95.8400011062622%\n",
      "Epoch 8/10, Loss: 0.0067688338458538055 Accuracy: 96.46999835968018%\n",
      "Epoch 9/10, Loss: 0.005955161526799202 Accuracy: 96.39000296592712%\n",
      "Epoch 10/10, Loss: 0.005352318752557039 Accuracy: 96.24000191688538%\n",
      "Epoch 1/1, Loss: 0.004458699841052294 Accuracy: 97.3800003528595%\n",
      "Accuracy: 97.22999572753906%\n",
      "iteration 3 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.003939295653253794 Accuracy: 84.9399983882904%\n",
      "Epoch 1/10, Loss: 0.0017660396406427026 Accuracy: 92.14000105857849%\n",
      "Epoch 2/10, Loss: 0.0034001083113253117 Accuracy: 92.61999726295471%\n",
      "Epoch 3/10, Loss: 0.006428918801248074 Accuracy: 94.2900002002716%\n",
      "Epoch 4/10, Loss: 0.008014858700335026 Accuracy: 94.73999738693237%\n",
      "Epoch 5/10, Loss: 0.009525499306619167 Accuracy: 92.35000014305115%\n",
      "Epoch 6/10, Loss: 0.009427627548575401 Accuracy: 94.48999762535095%\n",
      "Epoch 7/10, Loss: 0.007770647294819355 Accuracy: 96.24000191688538%\n",
      "Epoch 8/10, Loss: 0.0068091959692537785 Accuracy: 95.27999758720398%\n",
      "Epoch 9/10, Loss: 0.005968174897134304 Accuracy: 96.42999768257141%\n",
      "Epoch 10/10, Loss: 0.005783243104815483 Accuracy: 96.07999920845032%\n",
      "Epoch 1/1, Loss: 0.004417418502271175 Accuracy: 96.88000082969666%\n",
      "Accuracy: 96.93000030517578%\n",
      "iteration 4 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.003462359542027116 Accuracy: 85.68000197410583%\n",
      "Epoch 1/10, Loss: 0.001815054565668106 Accuracy: 93.12999844551086%\n",
      "Epoch 2/10, Loss: 0.0030757931526750326 Accuracy: 93.58999729156494%\n",
      "Epoch 3/10, Loss: 0.006328131537884474 Accuracy: 94.13999915122986%\n",
      "Epoch 4/10, Loss: 0.008013196289539337 Accuracy: 93.51000189781189%\n",
      "Epoch 5/10, Loss: 0.009412209503352642 Accuracy: 93.86000037193298%\n",
      "Epoch 6/10, Loss: 0.010319170542061329 Accuracy: 95.13000249862671%\n",
      "Epoch 7/10, Loss: 0.008727882988750935 Accuracy: 95.14999985694885%\n",
      "Epoch 8/10, Loss: 0.006763060111552477 Accuracy: 96.0099995136261%\n",
      "Epoch 9/10, Loss: 0.006407273467630148 Accuracy: 96.29999995231628%\n",
      "Epoch 10/10, Loss: 0.005625494755804539 Accuracy: 95.74999809265137%\n",
      "Epoch 1/1, Loss: 0.004792502149939537 Accuracy: 96.72999978065491%\n",
      "Accuracy: 96.75%\n",
      "iteration 5 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.003913940861821175 Accuracy: 84.78000164031982%\n",
      "Epoch 1/10, Loss: 0.002115756506100297 Accuracy: 92.11000204086304%\n",
      "Epoch 2/10, Loss: 0.004061195533722639 Accuracy: 92.36999750137329%\n",
      "Epoch 3/10, Loss: 0.007191747892647982 Accuracy: 94.4100022315979%\n",
      "Epoch 4/10, Loss: 0.00832380075007677 Accuracy: 93.0400013923645%\n",
      "Epoch 5/10, Loss: 0.008693426847457886 Accuracy: 94.80000138282776%\n",
      "Epoch 6/10, Loss: 0.009093689732253551 Accuracy: 95.10999917984009%\n",
      "Epoch 7/10, Loss: 0.007507686503231525 Accuracy: 94.8199987411499%\n",
      "Epoch 8/10, Loss: 0.006770360749214888 Accuracy: 96.16000056266785%\n",
      "Epoch 9/10, Loss: 0.006116277538239956 Accuracy: 95.81999778747559%\n",
      "Epoch 10/10, Loss: 0.005664189346134663 Accuracy: 96.03000283241272%\n",
      "Epoch 1/1, Loss: 0.004467404913157225 Accuracy: 97.00000286102295%\n",
      "Accuracy: 96.81999969482422%\n",
      "iteration 6 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.003764668945223093 Accuracy: 84.3500018119812%\n",
      "Epoch 1/10, Loss: 0.0021679983474314213 Accuracy: 92.5000011920929%\n",
      "Epoch 2/10, Loss: 0.004469166975468397 Accuracy: 94.30999755859375%\n",
      "Epoch 3/10, Loss: 0.0073276301845908165 Accuracy: 93.91999840736389%\n",
      "Epoch 4/10, Loss: 0.008336933329701424 Accuracy: 93.69999766349792%\n",
      "Epoch 5/10, Loss: 0.008788897655904293 Accuracy: 92.90000200271606%\n",
      "Epoch 6/10, Loss: 0.009106412529945374 Accuracy: 95.31000256538391%\n",
      "Epoch 7/10, Loss: 0.007495858706533909 Accuracy: 95.01000046730042%\n",
      "Epoch 8/10, Loss: 0.006598066072911024 Accuracy: 96.27000093460083%\n",
      "Epoch 9/10, Loss: 0.005992761813104153 Accuracy: 96.27000093460083%\n",
      "Epoch 10/10, Loss: 0.005482994485646486 Accuracy: 96.56000137329102%\n",
      "Epoch 1/1, Loss: 0.004534377716481686 Accuracy: 97.18999862670898%\n",
      "Accuracy: 97.18000030517578%\n",
      "iteration 7 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.003776917466893792 Accuracy: 85.79999804496765%\n",
      "Epoch 1/10, Loss: 0.0020071391481906176 Accuracy: 91.93999767303467%\n",
      "Epoch 2/10, Loss: 0.0034152588341385126 Accuracy: 93.12999844551086%\n",
      "Epoch 3/10, Loss: 0.005651445128023624 Accuracy: 93.36000084877014%\n",
      "Epoch 4/10, Loss: 0.008034106343984604 Accuracy: 93.76999735832214%\n",
      "Epoch 5/10, Loss: 0.009814869612455368 Accuracy: 94.38999891281128%\n",
      "Epoch 6/10, Loss: 0.0095740407705307 Accuracy: 94.67999935150146%\n",
      "Epoch 7/10, Loss: 0.007714437786489725 Accuracy: 95.8400011062622%\n",
      "Epoch 8/10, Loss: 0.006707406602799892 Accuracy: 95.5299973487854%\n",
      "Epoch 9/10, Loss: 0.006168107967823744 Accuracy: 95.45999765396118%\n",
      "Epoch 10/10, Loss: 0.005518277175724506 Accuracy: 96.60999774932861%\n",
      "Epoch 1/1, Loss: 0.004306863062083721 Accuracy: 97.07000255584717%\n",
      "Accuracy: 97.25%\n",
      "iteration 8 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.0036162585020065308 Accuracy: 86.05999946594238%\n",
      "Epoch 1/10, Loss: 0.0015264819376170635 Accuracy: 92.1500027179718%\n",
      "Epoch 2/10, Loss: 0.00268511357717216 Accuracy: 94.23999786376953%\n",
      "Epoch 3/10, Loss: 0.004502410069108009 Accuracy: 93.69000196456909%\n",
      "Epoch 4/10, Loss: 0.007360626943409443 Accuracy: 94.12000179290771%\n",
      "Epoch 5/10, Loss: 0.008077253587543964 Accuracy: 93.47000122070312%\n",
      "Epoch 6/10, Loss: 0.009267857298254967 Accuracy: 95.02999782562256%\n",
      "Epoch 7/10, Loss: 0.007685676217079163 Accuracy: 96.35000228881836%\n",
      "Epoch 8/10, Loss: 0.00697805592790246 Accuracy: 96.45000100135803%\n",
      "Epoch 9/10, Loss: 0.005944206379354 Accuracy: 96.64000272750854%\n",
      "Epoch 10/10, Loss: 0.005257550626993179 Accuracy: 97.02000021934509%\n",
      "Epoch 1/1, Loss: 0.003956648986786604 Accuracy: 97.2599983215332%\n",
      "Accuracy: 97.18000030517578%\n",
      "iteration 9 ... \n",
      "\n",
      "\n",
      "Epoch 1/1, Loss: 0.003784911008551717 Accuracy: 85.61999797821045%\n",
      "Epoch 1/10, Loss: 0.0014958715764805675 Accuracy: 92.11999773979187%\n",
      "Epoch 2/10, Loss: 0.0025408840738236904 Accuracy: 93.80000233650208%\n",
      "Epoch 3/10, Loss: 0.0051644365303218365 Accuracy: 93.90000104904175%\n",
      "Epoch 4/10, Loss: 0.007625467609614134 Accuracy: 94.78999972343445%\n",
      "Epoch 5/10, Loss: 0.00855984352529049 Accuracy: 94.20999884605408%\n",
      "Epoch 6/10, Loss: 0.008825969882309437 Accuracy: 95.14999985694885%\n",
      "Epoch 7/10, Loss: 0.00808249693363905 Accuracy: 95.169997215271%\n",
      "Epoch 8/10, Loss: 0.006895097438246012 Accuracy: 95.93999981880188%\n",
      "Epoch 9/10, Loss: 0.005759659688919783 Accuracy: 96.13999724388123%\n",
      "Epoch 10/10, Loss: 0.005467931739985943 Accuracy: 96.8500018119812%\n",
      "Epoch 1/1, Loss: 0.003946162760257721 Accuracy: 97.07000255584717%\n",
      "Accuracy: 97.33000183105469%\n",
      "Accuracy: 97.0989990234375%\n"
     ]
    }
   ],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Create and train the neural network\n",
    "qlenet = QLeNet(input_shape=input_shape, output_size=y_train.shape[-1], batch_size=256)\n",
    "\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    print(f\"iteration {i} ... \\n\\n\")\n",
    "    \n",
    "    # load pre-trained model\n",
    "    qlenet.load_layers_from_model(lenet)\n",
    "    qlenet.freeze_conv = True\n",
    "    # restart\n",
    "    qlenet.restart_fc_layers()\n",
    "    \n",
    "\n",
    "    # finetune the dnn\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, x_val=x_test, y_val=y_test)\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000100, num_epochs=10, x_val=x_test, y_val=y_test)\n",
    "    qlenet.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, x_val=x_test, y_val=y_test)\n",
    "    \n",
    "    \n",
    "    # predict finetuned\n",
    "    y_pred = qlenet.predict(x_test, 256)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    mean_acc += accuracy\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training last layer po2 from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.utils import model_quantize, model_save_quantized_weights\n",
    "from qkeras import *\n",
    "\n",
    "\n",
    "# add one relu layer after input\n",
    "x = x_in = keras.layers.Input((28,28,1))\n",
    "x = keras.layers.ReLU()(x)\n",
    "for l in lenet.layers[1:]:\n",
    "    x = l(x)\n",
    "\n",
    "\n",
    "lenet = keras.Model(inputs=[x_in], outputs=[x])\n",
    "lenet.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "quantizer_config = {        \n",
    "    \"QConv2D\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QDense\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QActivation\": { \"relu\": \"quantized_relu_po2(4,1,use_stochastic_rounding=True)\" },    \n",
    "}\n",
    "\n",
    "qmodel2 = model_quantize(lenet, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "qmodel2.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    print(\"iteratoin\", i, \"...\\n\")\n",
    "\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(lenet, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "    \n",
    "    # freeze and restart layer weights\n",
    "    for l in qmodel2.layers:\n",
    "        if isinstance(l, QConv2D):\n",
    "            l.trainable = False\n",
    "        if isinstance(l, QDense):            \n",
    "            w = keras.initializers.GlorotNormal()(l.weights[0].shape)            \n",
    "            b = tf.zeros_like(l.weights[1])            \n",
    "            l.set_weights([w, b])\n",
    "\n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)                \n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=10, validation_data=(x_test, y_test), validation_freq=1)                \n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)                \n",
    "\n",
    "\n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "    \n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
