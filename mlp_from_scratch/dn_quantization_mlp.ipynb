{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train mlp on MNIST using fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dataset import load_mnist\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "\n",
    "# load dataset\n",
    "x_train, y_train, x_test, y_test = load_mnist(flatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_in = keras.layers.Input((784))\n",
    "x = keras.layers.Dense(256)(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Dense(256)(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Dense(10)(x)\n",
    "\n",
    "mlp = keras.Model(inputs=[x_in], outputs=[x])\n",
    "mlp.summary()\n",
    "mlp.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "hist = mlp.fit(x_train, y_train, 256, 10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNetwork import QNeuralNetworkWithScale\n",
    "import Activations\n",
    "import FullyConnectedLayer \n",
    "\n",
    "\n",
    "# Define neural network inputnoutput\n",
    "input_size = x_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# Create and train the neural network\n",
    "neural_network_with_scale = QNeuralNetworkWithScale(input_size, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep nibble direct quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = 0\n",
    "for i in range(100):\n",
    "    print(f\"iteration {i} ... \\n\\n\")\n",
    "    \n",
    "    neural_network_with_scale.load_layers_from_model(mlp)\n",
    "\n",
    "    y_pred = neural_network_with_scale.predict(x_test, 256)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    mean_acc += accuracy\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "mean_acc /= 100\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep nibble finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    print(f\"iteration {i} ... \\n\\n\")\n",
    "    \n",
    "    neural_network_with_scale.load_layers_from_model(mlp)\n",
    "\n",
    "    # finetune the dnn\n",
    "    neural_network_with_scale.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, batch_size=256, x_val=x_test, y_val=y_test)\n",
    "    \n",
    "    # predict finetuned\n",
    "    y_pred = neural_network_with_scale.predict(x_test, 256)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(y_pred == tf.argmax(y_test, axis=1), tf.float32))\n",
    "    mean_acc += accuracy\n",
    "    print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PO2 direct quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one relu layer after input\n",
    "x = x_in = keras.layers.Input((784))\n",
    "x = keras.layers.ReLU()(x)\n",
    "for l in mlp.layers[1:]:\n",
    "    x = l(x)\n",
    "\n",
    "\n",
    "mlp = keras.Model(inputs=[x_in], outputs=[x])\n",
    "mlp.summary()\n",
    "mlp.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras.utils import model_quantize, model_save_quantized_weights\n",
    "from qkeras import *\n",
    "\n",
    "\n",
    "quantizer_config = {        \n",
    "    \"QDense\": {\n",
    "        \"kernel_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\",\n",
    "        \"bias_quantizer\": \"quantized_po2(4,1,use_stochastic_rounding=True)\"\n",
    "    },\n",
    "    \"QActivation\": { \"relu\": \"quantized_relu_po2(4,1,use_stochastic_rounding=True)\" },    \n",
    "}\n",
    "\n",
    "qmodel2 = model_quantize(mlp, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "qmodel2.summary()\n",
    "\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(mlp, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    \n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "\n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PO2 finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "mean_acc = 0\n",
    "for i in range(10):\n",
    "    # quantize the mlp model\n",
    "    qmodel2 = model_quantize(mlp, quantizer_config, activation_bits=4, transfer_weights=True)    \n",
    "\n",
    "    # compile \n",
    "    qmodel2.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "\n",
    "    # train\n",
    "    history = qmodel2.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=1, validation_data=(x_test, y_test), validation_freq=1)    \n",
    "    \n",
    "    model_save_quantized_weights(qmodel2, \"qmodels/qmlp_po2/\")\n",
    "\n",
    "    # evaluate\n",
    "    loss, acc = qmodel2.evaluate(x_test, y_test)\n",
    "\n",
    "    mean_acc += acc\n",
    "\n",
    "\n",
    "mean_acc /= 10\n",
    "print(f\"Accuracy: {mean_acc * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep nibble training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dataset import load_mnist\n",
    "from NeuralNetwork import QNeuralNetworkWithScale\n",
    "import Activations\n",
    "import FullyConnectedLayer\n",
    "\n",
    "\n",
    "# load dataset\n",
    "x_train, y_train, x_test, y_test = load_mnist(flatten=True)\n",
    "\n",
    "# Define neural network inputnoutput\n",
    "input_size = x_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "acc_hist = []\n",
    "for i in range(100):\n",
    "    print(f\"iteration {i} ... \\n\")\n",
    "\n",
    "    # Create and train the neural network\n",
    "    neural_network_with_scale = QNeuralNetworkWithScale(input_size, output_size)\n",
    "\n",
    "    # train the nn\n",
    "    neural_network_with_scale.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, batch_size=256, x_val=x_test, y_val=y_test)\n",
    "    neural_network_with_scale.train(x_train, y_train, learning_rate=0.000100, num_epochs=10, batch_size=256, x_val=x_test, y_val=y_test)\n",
    "    neural_network_with_scale.train(x_train, y_train, learning_rate=0.000010, num_epochs=1, batch_size=256, x_val=x_test, y_val=y_test)\n",
    "\n",
    "    acc_hist.append(neural_network_with_scale.acc_hist[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PO2 training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " q_activation_5 (QActivation  (None, 784)              0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " q_dense_3 (QDense)          (None, 256)               200960    \n",
      "                                                                 \n",
      " q_activation_6 (QActivation  (None, 256)              0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " q_dense_4 (QDense)          (None, 256)               65792     \n",
      "                                                                 \n",
      " q_activation_7 (QActivation  (None, 256)              0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " q_dense_5 (QDense)          (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 2.2527 - accuracy: 0.1743 - val_loss: 2.0958 - val_accuracy: 0.2967\n",
      "Epoch 1/10\n",
      "235/235 [==============================] - 2s 7ms/step - loss: 1.3385 - accuracy: 0.6849 - val_loss: 0.8906 - val_accuracy: 0.8095\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.7325 - accuracy: 0.8382 - val_loss: 0.5906 - val_accuracy: 0.8668\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.5467 - accuracy: 0.8695 - val_loss: 0.4812 - val_accuracy: 0.8814\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4612 - accuracy: 0.8840 - val_loss: 0.4162 - val_accuracy: 0.8955\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.4059 - accuracy: 0.8954 - val_loss: 0.3731 - val_accuracy: 0.9026\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3704 - accuracy: 0.9017 - val_loss: 0.3477 - val_accuracy: 0.9094\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3459 - accuracy: 0.9068 - val_loss: 0.3233 - val_accuracy: 0.9131\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.9110 - val_loss: 0.3053 - val_accuracy: 0.9159\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.3072 - accuracy: 0.9152 - val_loss: 0.2903 - val_accuracy: 0.9196\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 0.2912 - accuracy: 0.9186 - val_loss: 0.2781 - val_accuracy: 0.9225\n",
      "235/235 [==============================] - 2s 6ms/step - loss: 0.2830 - accuracy: 0.9210 - val_loss: 0.2751 - val_accuracy: 0.9237\n"
     ]
    }
   ],
   "source": [
    "import qkeras\n",
    "from qkeras import *\n",
    "\n",
    "x = x_in = keras.layers.Input((784))\n",
    "x = qkeras.QActivation(\"quantized_relu_po2(4,1,use_stochastic_rounding=True)\")(x)\n",
    "x = qkeras.QDense(256, kernel_quantizer=quantized_po2(4,1,use_stochastic_rounding=True), bias_quantizer=quantized_po2(4,1,use_stochastic_rounding=True))(x)\n",
    "x = qkeras.QActivation(\"quantized_relu_po2(4,1,use_stochastic_rounding=True)\")(x)\n",
    "x = qkeras.QDense(256, kernel_quantizer=quantized_po2(4,1,use_stochastic_rounding=True), bias_quantizer=quantized_po2(4,1,use_stochastic_rounding=True))(x)\n",
    "x = qkeras.QActivation(\"quantized_relu_po2(4,1,use_stochastic_rounding=True)\")(x)\n",
    "x = qkeras.QDense(10, kernel_quantizer=quantized_po2(4,1,use_stochastic_rounding=True), bias_quantizer=quantized_po2(4,1,use_stochastic_rounding=True))(x)\n",
    "\n",
    "mlp = keras.Model(inputs=[x_in], outputs=[x])\n",
    "mlp.summary()\n",
    "mlp.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "hist = mlp.fit(x_train, y_train, 256, 1, validation_data=(x_test, y_test))\n",
    "mlp.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "hist = mlp.fit(x_train, y_train, 256, 10, validation_data=(x_test, y_test))\n",
    "mlp.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "hist = mlp.fit(x_train, y_train, 256, 1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FP32 training from scratch (same number of epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 784)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               200960    \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                2570      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 2.2770 - accuracy: 0.1738 - val_loss: 2.2085 - val_accuracy: 0.2831\n",
      "Epoch 1/10\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 1.5824 - accuracy: 0.6492 - val_loss: 0.9827 - val_accuracy: 0.8129\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.7520 - accuracy: 0.8358 - val_loss: 0.5703 - val_accuracy: 0.8646\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.8691 - val_loss: 0.4473 - val_accuracy: 0.8843\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8840 - val_loss: 0.3888 - val_accuracy: 0.8948\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8935 - val_loss: 0.3558 - val_accuracy: 0.9020\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.3624 - accuracy: 0.8986 - val_loss: 0.3331 - val_accuracy: 0.9073\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.3418 - accuracy: 0.9042 - val_loss: 0.3171 - val_accuracy: 0.9099\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.3254 - accuracy: 0.9084 - val_loss: 0.3040 - val_accuracy: 0.9135\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.3123 - accuracy: 0.9117 - val_loss: 0.2916 - val_accuracy: 0.9176\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 0s 2ms/step - loss: 0.3008 - accuracy: 0.9147 - val_loss: 0.2835 - val_accuracy: 0.9192\n",
      "235/235 [==============================] - 1s 2ms/step - loss: 0.2940 - accuracy: 0.9172 - val_loss: 0.2812 - val_accuracy: 0.9210\n"
     ]
    }
   ],
   "source": [
    "x = x_in = keras.layers.Input((784))\n",
    "x = keras.layers.Dense(256)(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Dense(256)(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Dense(10)(x)\n",
    "\n",
    "mlp = keras.Model(inputs=[x_in], outputs=[x])\n",
    "mlp.summary()\n",
    "mlp.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "hist = mlp.fit(x_train, y_train, 256, 1, validation_data=(x_test, y_test))\n",
    "mlp.compile(optimizer=keras.optimizers.SGD(0.01), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "hist = mlp.fit(x_train, y_train, 256, 10, validation_data=(x_test, y_test))\n",
    "mlp.compile(optimizer=keras.optimizers.SGD(0.001), loss=keras.losses.CategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "hist = mlp.fit(x_train, y_train, 256, 1, validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
